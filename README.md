# Time-Series-Forecasting-Model-Using-Python
Sá»­ Dá»¥ng MÃ´ HÃ¬nh Dá»± BÃ¡o Chuá»—i Thá»i Gian Báº±ng Python
MÃ´ hÃ¬nh LSTM
MÃ´ hÃ¬nh LSTM (Long Short-Term Memory) lÃ  má»™t dáº¡ng Ä‘áº·c biá»‡t cá»§a máº¡ng nÆ¡-ron há»“i quy (RNN) Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u chuá»—i vÃ  giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient trong RNN.
Má»™t LSTM cÃ³ cáº¥u trÃºc tÆ°Æ¡ng tá»± nhÆ° RNN, nhÆ°ng nÃ³ cÃ³ thÃªm cÃ¡c cÆ¡ cháº¿ Ä‘áº·c biá»‡t nhÆ° cá»•ng (gates) Ä‘á»ƒ Ä‘iá»u chá»‰nh thÃ´ng tin Ä‘Æ°á»£c lÆ°u trá»¯ vÃ  truyá»n qua láº¡i trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n.
CÃ¡c cá»•ng trong LSTM bao gá»“m:
	 Cá»•ng quÃªn (Forget gate): XÃ¡c Ä‘á»‹nh thÃ´ng tin nÃ o trong tráº¡ng thÃ¡i trÆ°á»›c Ä‘Ã³ cáº§n bá»‹ lÃ£ng quÃªn. NÃ³ quyáº¿t Ä‘á»‹nh giá»¯ láº¡i thÃ´ng tin nÃ o vÃ  bá» qua thÃ´ng tin nÃ o tá»« quÃ¡ khá»©.
	 Cá»•ng Ä‘áº§u vÃ o (Input gate): XÃ¡c Ä‘á»‹nh thÃ´ng tin má»›i nÃ o sáº½ Ä‘Æ°á»£c lÆ°u trá»¯ trong tráº¡ng thÃ¡i tiáº¿p theo. NÃ³ quyáº¿t Ä‘á»‹nh thÃ´ng tin má»›i nÃ o sáº½ Ä‘Æ°á»£c cáº­p nháº­t tá»« Ä‘áº§u vÃ o hiá»‡n táº¡i.
	 Cá»•ng Ä‘áº§u ra (Output gate): XÃ¡c Ä‘á»‹nh pháº§n nÃ o cá»§a tráº¡ng thÃ¡i hiá»‡n táº¡i sáº½ lÃ  Ä‘áº§u ra cá»§a LSTM. NÃ³ quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o sáº½ Ä‘Æ°á»£c Ä‘Æ°a ra tá»« tráº¡ng thÃ¡i hiá»‡n táº¡i.
	 Tráº¡ng thÃ¡i áº©n (Cell state): LÃ  má»™t bá»™ nhá»› dÃ i háº¡n, lÆ°u trá»¯ thÃ´ng tin tá»« quÃ¡ khá»© vÃ  truyá»n thÃ´ng tin tá»›i cÃ¡c bÆ°á»›c tÃ­nh toÃ¡n tiáº¿p theo trong chuá»—i.
	 Vá»›i cÃ¡c cá»•ng nÃ y, LSTM cÃ³ kháº£ nÄƒng lÆ°u trá»¯ thÃ´ng tin quan trá»ng tá»« quÃ¡ khá»© vÃ  loáº¡i bá» thÃ´ng tin khÃ´ng quan trá»ng. Äiá»u nÃ y giÃºp LSTM giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient vÃ  cÃ³ kháº£ nÄƒng xá»­ lÃ½ Ä‘Æ°á»£c cÃ¡c chuá»—i dÃ i vÃ  tÆ°Æ¡ng quan phá»©c táº¡p hÆ¡n so vá»›i RNN thÃ´ng thÆ°á»ng.
	 MÃ´ hÃ¬nh LSTM Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c lÄ©nh vá»±c nhÆ° xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, dá»‹ch mÃ¡y, nháº­n dáº¡ng giá»ng nÃ³i, dá»± bÃ¡o chuá»—i thá»i gian vÃ  nhiá»u á»©ng dá»¥ng khÃ¡c liÃªn quan Ä‘áº¿n dá»¯ liá»‡u chuá»—i.
TÃ³m láº¡i, mÃ´ hÃ¬nh LSTM lÃ  má»™t dáº¡ng Ä‘áº·c biá»‡t cá»§a máº¡ng nÆ¡-ron há»“i quy, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u chuá»—i vÃ  giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient. NÃ³ sá»­ dá»¥ng cÃ¡c cá»•ng Ä‘á»ƒ Ä‘iá»u chá»‰nh thÃ´ng tin vÃ  tráº¡ng thÃ¡i trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n, giÃºp nÃ³ lÆ°u trá»¯ thÃ´ng tin quan trá»ng vÃ  loáº¡i bá» thÃ´ng tin khÃ´ng quan trá»ng. LSTM Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  má»™t cÃ´ng cá»¥ máº¡nh máº½ trong viá»‡c xá»­ lÃ½ vÃ  dá»± Ä‘oÃ¡n cÃ¡c dá»¯ liá»‡u chuá»—i phá»©c táº¡p.
 
HÃ¬nh 1 4 Cáº¥u trÃºc mÃ´ hÃ¬nh LSTM

Output:ct,ht,ta gá»i c lÃ  cell state,t lÃ  hidden state.
Input: ct-1,ht-1,xt.Trong Ä‘Ã³ xt lÃ  input á»Ÿ state thá»© t cá»§a model.
         Äá»c biá»ƒu Ä‘á»“ á»Ÿ trÃªn ta cÃ³ thá»ƒ tháº¥y kÃ­ hiá»‡u Ïƒ, tanh cÃ³ nghÄ©a lÃ  á»Ÿ bÆ°á»›c Ä‘áº¥y dÃ¹ng hÃ m sigma , tanh.
PhÃ©p nhÃ¢n á»Ÿ Ä‘Ã¢y lÃ  phÃ©p nhÃ¢n tá»«ng pháº§n tá»­, phÃ©p cá»™ng lÃ  cá»™ng ma tráº­n.
Ft,it,ot tÆ°Æ¡ng á»©ng cá»›i forget gate,input gate,output gate:
Forget gate: ft=Ïƒ(Uğ‘“*xğ‘¡+Wğ‘“*hğ‘¡âˆ’1+bğ‘“)
         Input gate: it=Ïƒ(Uğ‘–*xğ‘¡+Wğ‘–*hğ‘¡âˆ’1+bi)
         Output gate: ot= Ïƒ(Uo*xt+Wo*ht-1+bo)
Nháº­n xÃ©t 0<ft,it,ot<1;bf,bi,bo lÃ  cÃ¡c há»‡ sá»‘ bias,há»‡ sá»‘ W,U giá»‘ng nhÆ° bÃ i RNN 
c = tanhâ¡(Uc * xt+Wc*ht-1+bc)
ct = ft*ct-1+it*ct, forget gate quyáº¿t Ä‘á»‹nh xem láº¥y bao nhiÃªu tá»« cell state trÆ°á»›c vÃ  input gate sáº½ quyáº¿t Ä‘á»‹nh láº¥y bao nhiÃªu tá»« input state vÃ  hidden layer cá»§a layer trÆ°á»›c.
 ht = ot*tanhâ¡(ct) ,output gate quyáº¿t Ä‘á»‹nh xem cáº§n láº¥y bao nhiÃªu tá»« cell state Ä‘á»ƒ trá»Ÿ thÃ nh output cá»§a hidden state. NgoÃ i ra ht cÅ©ng Ä‘c dÃ¹ng Ä‘á»ƒ tÃ­nh ra output yt cho state t.
Nháº­n xÃ©t: ht,ct khÃ¡ giá»‘ng vá»›i RNN,nÃªn model cÃ³ short táº»m memory.Trong khi Ä‘Ã³ ct giá»‘ng nhÆ° má»™t bÄƒng chuyá»n á»Ÿ trÃªn mÃ´ hÃ¬nh RNN, thÃ´ng tin nÃ o cáº§n quan trá»ng vÃ  dÃ¹ng á»Ÿ sau sáº½ Ä‘Æ°á»£c gá»­i vÃ o dÃ¹ng khi cáº§n=>cÃ³ thá»ƒ mang thÃ´ng tin tá»« Ä‘i xa.Do Ä‘Ã³ mÃ´ hÃ¬nh LSTM cÃ³ cáº£ short term memory vÃ  long term memory.
